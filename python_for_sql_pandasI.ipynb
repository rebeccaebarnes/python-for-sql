{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for SQL: Pandas\n",
    "\n",
    "We've spent some time getting familiar with the basic functionality of Python - now it's time to focus more on some analytics and discover just how much Python can parallel SQL, with the [pandas](https://pandas.pydata.org/) library. A library is an \"addon\" to the base Python package, providing enhanced functionality to the existing options. According to [its site on PyPi](https://pypi.org/project/pandas/) (the Python Package Index), the goal of the developers for pandas is that it will become, \"the most powerful and flexible open source data analysis / manipulation tool available in any language.\"\n",
    "\n",
    "**Note:** Now that we are getting into more detail, don't forget that you can add new cells by clicking the `+` button above and move them around using the up/down arrows. You can then convert them to markdown cells. (For a refresher on how to do this, you can check my [blog post](https://rebeccaebarnes.github.io/2019/08/25/python-sql-start).)\n",
    "\n",
    "## Setting Up a pandas Environment\n",
    "For us to use pandas there is some setup that we'll need to implement. We're going to set up a **conda environment**. An environment is a contained space that allows us to install and control the versions of Python and other libraries that we use. This allows us to manage an incompatbilities between library versions while knowing exactly how we're expecting our code to perform. If we're working with others, it also ensures that everyone is the same coding environment.\n",
    "\n",
    "### Anaconda Prompt\n",
    "When installing Anaconda, one of the the programs that was included in installation was Anaconda Prompt. We will use this to set up our environment. There are two ways for us create a new environment.\n",
    "1. Install from an existing .yaml file\n",
    "2. Complete an installation from scratch\n",
    "\n",
    "I'm going to walk through both, but either will work. Installing from the yaml file is more straight forward.\n",
    "\n",
    "#### `.yaml` Installation\n",
    "The `.yaml` (pronounced yah-mul) file contains all of the libraries and dependencies that I have used to create my pandas environment and allows you to replicate it exactly. This is what you can use to share your environment with another to collaborate on the same project.\n",
    "1. Open Anaconda Prompt and check the default directory that is shown. Mine says `(base) C:\\Users\\rbarnes`. \n",
    "2. Make sure that you've downloaded the `data_analysis.yaml` file from the GitHub repo and move it to the default folder shown in Anaconda Prompt.\n",
    "3. Type out the following and press Enter: `conda env create -f data_analysis.yaml`.\n",
    "\n",
    "This is everything you need to do to create the new environment and this is all you need to do for now.\n",
    "\n",
    "#### Manual Installation\n",
    "If you would like to create an environment from scratch, here's how we would do that.\n",
    "\n",
    "1. Open Anaconda Prompt and type the following: `conda create -n data_analysis python=3.7 pandas ipykernel`. Check out the image below for an explanation of the functionality - we're going to create the environment with Python 3.7, pandas and ipykernel. You will be prompted at different points to confirm whether you want to continue, press 'y' and Enter.\n",
    "\n",
    "![env_create](img/env_creation.png)\n",
    "\n",
    "2. From here we're going to activate our new environment. Type `conda activate data_analysis` and press Enter. You'll notice that the name `(base)` changes to `(data_analysis)`. \n",
    "3. Once we're inside the environment we're going to install an additional library, matplotlib. Type `conda install -c conda-forge matplotlib`. Once again, say 'y' to any prompts. This will install the matplotlib library but make sure the most up to date version is installed.\n",
    "4. Finally, we want to make sure that the environment is visible within Jupyter Notebooks. To do so we need to enter the following: `python -m ipykernel install --name data_analysis --display-name \"Python (data_analysis)\"`. \n",
    "\n",
    "Once you have finished creating the environment you can close Anaconda Prompt.\n",
    "\n",
    "### Jupyter Notebooks\n",
    "Now, when we open Jupyter Notebooks, there should be an option to create a new notebook from the environment that you just created. Find the `New` button in the top right-hand corner and select the environment from the dropdown. This is what you should do any time you want to start a new notebook that has access to \n",
    "\n",
    "![new_nb](img/new_notebook.png)\n",
    "\n",
    "If you are following along in this notebook, you will also want to change this environment you are using, because the default environment will not have access to pandas. \n",
    "\n",
    "We do this by changing _kernels_. To change the kernel of the notebook, find the `Kernel` menu in the navigation bar at the top of the notebook, and find the Change Kernel option (likely at the bottom of the list). Hover over and select the \"Python (data_analysis)\" environment from the list.\n",
    "\n",
    "We are now ready to get started!!\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The first thing that needs to be done to get started working with pandas is to import it. We import libraries in the same way as we import modules. Run the cell below to import the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that I included an extra component to this import. I included `as pd` to the end. Adding `as pd` to the end of the import statement is called aliasing. This means that any time I need to reference the pandas library in the name of the function/object I am calling, I only need to write `pd.function_name` rather than `pandas.function_name` - I use the alias. It is possible to give any libary you import whatever alias you wish, but this is the standard alias for pandas and is worth using so that your code works easily with others.\n",
    "\n",
    "It is also best practice to include all of your import statements in the first cell that you run in your notebook. This allows anyone who is working through the notebook to easily know which libraries and modules you used. Even if you discover partway through the notebook that you need to import an additional library or notebook, you can just come back to that initial cell and rerun it to import the new library.\n",
    "\n",
    "For the rest of the notebook we will look at three key elements of SQL code and their comparisons in pandas (more to come in future notebooks!).\n",
    "- FROM\n",
    "- SELECT\n",
    "- LIMIT\n",
    "- DISTINCT\n",
    "\n",
    "## FROM\n",
    "When pulling data in SQL, we will indicate which source table to read from using the `FROM` statement. When we're working with data in pandas we are reading it in from a source file. There are many different file types that pandas can read, but the most common file type from which to read and write to is a `.csv` file. We're going to use a file from [Kaggle](https://www.kaggle.com/ronitf/heart-disease-uci) about heart disease that originall came from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). If you're following along with these notebooks, make sure you've got the `heart.csv` file in the same folder as this notebook.\n",
    "\n",
    "### `read_csv`\n",
    "\n",
    "Run the cell below to read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "df = pd.read_csv('heart.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there are two lines of code that I've used here. The `pd.read_csv()` is the function we use to read in the file, with the file name as the argument for the function. (The function has a number of optional arguments but we don't need to use any of those for the time being) We assigned the results of reading the csv to a variable called `df`.\n",
    "\n",
    "What we've read in is called a `DataFrame` (hence the abbreviated variable name `df`). It's one of two core data types in pandas. It functions very similar to a table in SQL or a spreadsheet in Excel. It is made up of columns and rows and we can perform operations on either the columns or rows in one go. \n",
    "\n",
    "One of the conveniences of Python is that once we've read in our data and assigned it to a variable we always have it available to access. We don't have to keep re-reading in the data, we can use it for many different operations from the same variable.\n",
    "\n",
    "### `head`\n",
    "\n",
    "The other code line is `df.head()`. This is a method that is available for seeing a preview of the DataFrame that we've read in. Similarly to how we would see a preview of the table we pulled from, in Python we can use `.head()` to see the first so many rows. By default, we will see five rows, but we can add a number as an argument to adjust the number of rows we see. \n",
    "\n",
    "**1\\.** What would you use to see just the first row of the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display just the first row of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `shape` \n",
    "\n",
    "Another thing that we would typically see in the results returned by SQL is the number of rows that were returned. We can do similarly with pandas by calling the `shape` attribute. An attribute is a component of Python objects that we haven't covered yet. With Python objects we can store extra information about the variable other than just what it is storing. These are called attributes and we can access them with a `.` and their name. \n",
    "\n",
    "At this point in time, knowing the exact details of how attributes are used and function is not so critical, but it does explain why we leave off the brackets when we use `shape`. A function/method requires the brackets, but accessing an attribute does not. \n",
    "\n",
    "Run the cell below to view what is in the `shape` attribute for our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see what's in the shape attribute\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about the dimensions of dataframes we always refer to the rows first, and then the columns. So our dataframe has 303 rows and 14 columns.\n",
    "\n",
    "**2\\.** Do you recognize the data type of what is stored in shape? If you just wanted to access the number of rows from `shape`, what could you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the number of rows in the dataframe from the shape attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `info`\n",
    "One other thing that we can typically do in SQL is to explore the data types of each of the columns in the table we are pulling from. We can do the same in pandas with the `.info()` method. \n",
    "\n",
    "**3\\.** Try running `.info()` on the dataframe to see what results you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the .info method on the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><span style=\"color:blue\">Click to see my result</span></summary>\n",
    "    \n",
    "![result1](img/df_info.png)\n",
    "\n",
    "</details>\n",
    "\n",
    "`info` provides us two key pieces of information. You will see that each row has `303 non-null` values. `info` will let us quickly explore whether any rows have missing values. You'll also see `int64` and `float64` at the end of each row. This is the data type that pandas has inferred for the column from the data that it read. For now, we can just talk about these as `int` and `float`. \n",
    "\n",
    "When it comes to the data type of a column, one thing to keep in mind is that in pandas, while you _should_ just have one data type in your column, it is not enforced in the same way that it is in SQL. If pandas reads in a column that appears to have mixed data types, it will warn you and you can investigate. For this data set it's not something that we need to manage, but it's worth keeping in mind. \n",
    "\n",
    "### `isna/isnull`\n",
    "`info` provides us with a bunch of helpful information in one go, but sometimes we just want to access one piece of information at a time. For example, we might want to look up whether we have missing information. For this we can use `.isna()` or `.isnull()`. Both provide the same result.\n",
    "\n",
    "**4\\.** Run both `.isna()` and `.isnull()` on our dataframe to confirm that they produce the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run isna on the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run isnull on the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you likely saw, both of these methods give us the results for every single cell in the dataframe. A result of False means that the cell isn't null. There are times when this is useful, but for our purposes, we'd probably prefer a summary. If we remember that we can add boolean values (False = 0, True = 1), this gives us a way to summarize the information. \n",
    "\n",
    "We can use `df.isna().sum()` to add up all the values in the columns. The result will indicate how many null values are in each column.\n",
    "\n",
    "**5\\.** How many missing values are in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the null values in each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Double click to edit and record your answer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did here demonstrates a key functionality in pandas. We can chain methods together, to act sequentially by adding a `.` and the name of the method. What we did above is the equivalent of this multi-step version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "null_values = df.isna()\n",
    "null_values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining methods together has pros and cons.\n",
    "\n",
    "**Pros**\n",
    "- When we chain methods together we can complete all of the actions in one step rather than having to assign the results to many \"interim\" variables that we just use to produce the end results.\n",
    "\n",
    "**Cons**\n",
    "- Especially when learning, it can be harder to read/understand what is happening in the chain (good code formatting can help with this)\n",
    "- If something goes wrong in the chain it can be harder to debug because it may be less clear where the error occurred. \n",
    "\n",
    "Essentially, there are times when chaining is helpful, but there can also be times when it's helpful to break the work into individual steps. Over time you'll get a handle on what works best for you.\n",
    "\n",
    "### `dtypes`\n",
    "\n",
    "In the same way that we might want access our missing/null values separately, at times we want to access our data types of our columns separately. To do this we can use the `dtypes` attribute.\n",
    "\n",
    "**6\\.** Display the data type information for your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to run your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT\n",
    "\n",
    "Now that we've read in our data, we want to be able to select specific columns and not deal with all of our data at once. If we want to access a column in our dataframe, we can use similar indexing methodology that we have used before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to show only the age column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll up (or run an additional cell to check), you will see that the first five results that we have are the same as the first five values in the `age` column when we used `head`. We can do this for any column in the dataframe. (Note: It is essential that we wrap any column name in quotes so that it is processed as a string. We'll get an error without them because Python will assume it's the name of a variable that we haven't yet created.)\n",
    "\n",
    "pandas actually gives you two ways to access your column names. Instead of the square brackets you can just use a `.` and then the name of the column without any quotation marks. You'll find both in use around the place but sometimes square brackets are preferred to make it clear that you are referring to a column.\n",
    "\n",
    "**7\\.** Display the age column using dot notation instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the age column using dot notation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One handy hint to remember when referencing your columns is that when using either notation to access your column names, you can use tab to either auto-complete the name, or give you prompts for options. Jupyter notebooks will also let you do this with your variable names. Type part of the name and then press tab and it will fill in the rest or give you options to pick from. \n",
    "\n",
    "Of course, in the same way that sometimes we select multiple columns in SQL, we can do the same thing with pandas. If we want to collect multiple columns, each of their names needs to be in a list and then we wrap them in the slicing square brackets.\n",
    "\n",
    "**8\\.** Create a list with column names for age, sex and target (You could call it `cols`). Then display these three columns. (In this case autocomplete won't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the columns age, sex, and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIMIT\n",
    "\n",
    "One thing that you may have noticed is that when we accessed the column, we accessed all of the values. Sometimes we don't want to access all of the values, we'd just like to access a few of them. While different SQL syntaxes have slightly different ways of limiting the number of results that are returned, all of them can do this in different ways. \n",
    "\n",
    "In pandas we can use one of two options `.iloc[]` and `.loc[]` where \"loc\" stands for location (and so is pronounced \"lock\") and \"i\" stands for index. The difference is that `loc` allows us to reference the parts of our tables using names and `iloc` allows us to reference the parts of our table using numbers. \n",
    "\n",
    "Both `loc` and `iloc` have a similar functionality. First, we reference the rows (we can use indexing or slicing), we add a comma (`,`), then we reference the columns (we can use indexing or slicing).\n",
    "\n",
    "**9\\.** Let's say that we wanted to reference the first value of the first column. Would we use `loc` or `iloc` to do this? Use the correct one to display this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first value of the first column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning it can be tricky to understand exactly what you're supposed to reference when using `loc` and `iloc`. The way to understand it is that every single row and column in our data frame can be referenced using either a name or a position. Let's look at something we've seen before to understand this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "datatypes = df.dtypes\n",
    "datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assigned the results of our column datatypes to a variable called `datatypes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "type(datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the other core object in pandas, a Series. Both DataFrames and Series in pandas have rows (DataFrames also have columns). All rows and columns in these pandas objects can be named with either names or numbers. As we've said, we can access these rows by their names or numbers.\n",
    "\n",
    "**10\\.** If you wanted to access the first row in the series using `iloc`, how would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11\\.** If you were wanting to access the first row in the series using `loc`, how would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series, the names of our rows are words and not numbers. Usually rows are named with numbers, but it's not always the case. And, every so often, these numbers match their position, but this also doesn't have to be the case. This means that we need to pay close attention to whether it's best to use `iloc` or `loc` when accessing our cells. \n",
    "\n",
    "As I said before, `loc` allows us to access the values by referencing the _names_ of their location. An example would be finding the value that is at the row called `5` and the column called `sex`. \n",
    "\n",
    "**12\\.** Find this value in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though `5` is the name of the index, we don't use quotation marks to make it a string. This is because DataFrame index values are named with integers by default. \n",
    "\n",
    "You can confirm this by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "df.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It is possible for your index to be named with strings that look like numbers, but it's uncommon.)\n",
    "\n",
    "Again, as discussed, `iloc` allows us to access values by referencing the _position_ of their location. In this case we would talk about finding the value in the 10th row in the 7th column.\n",
    "\n",
    "**13\\.** Find this value in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check your answer - did you remember the zero-indexing?\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "    <summary><span style=\"color:blue\">Click to see my code</span></summary>\n",
    "\n",
    "```python\n",
    "df.iloc[9, 6]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "To this point, we've just accessed the values from a single cell at a time. But it's completely possible to access values from multiple rows and/or columns. If we wanted to access multiple values from a list or a tuple, we would separate the beginning and end with a colon (`:`). We can do the same with `iloc` and `loc` for either our rows or our columns.\n",
    "\n",
    "**14\\.** You want to access the first five rows in the columns age, sex, and target. How would you do this? (This is a bit tricky, you'll need to think through what is the best method do to this. Explain your justification below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Double click to edit and add your reasoning for your methodology._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><span style=\"color:blue\">Check my code and explanation</span></summary>\n",
    "\n",
    "Here's my answer.\n",
    "\n",
    "```python\n",
    "df.loc[:4, ['age', 'sex', 'target']]\n",
    "```\n",
    "\n",
    "As you can see, I used `loc`. There is an argument that can be made for using either of `loc` or `iloc`. The argument for using `iloc` is that we are talking about the first five rows. This refers to a position instead of a name. But, we've been given the column names. If I wanted to use `iloc`, I'd need to find the positions of the columns as well. In this case, it's not that bad, but it can be easy to mess up a column position if there are many columns.\n",
    "\n",
    "Here's how I would have coded it if I was using `iloc`.\n",
    "\n",
    "```python\n",
    "df.iloc[:5, [0, 1, -1]]\n",
    "```\n",
    "\n",
    "The key difference that you should notice between these two piece of code is the difference in the number that I used to reference the rows. When I used `loc`, I referenced `4`, but when I used `iloc`, I referenced `5`. The reason for this is because of how each functions. When I use `loc` I'm referencing the _name_. I need to tell Python the exact names of the beginning and end positions and it will use those. However, with `iloc` I am referencing the _position_. `iloc` follows the same convention that Python normally uses in slicing by stopping at one _before_ the value indicated. \n",
    "\n",
    "I've also used a colon in the same way that we would in regular Python slicing. No value before the colon means \"everything up to this point\". I also just collected the names or positions of the columns into a list rather than assigning them to a variable first. I've also used negative numbers because I knew that `target` was the last column. \n",
    "\n",
    "</details>\n",
    "\n",
    "One thing to remember when using both `loc` and `iloc` is that you are always required to include details about the rows you need, but information about your columns is optional. This means that unless you say otherwise, pandas will give you all the column information for the rows you indicated. \n",
    "\n",
    "If you want to grab all of the rows for the columsns specified you can use a colon (`:`) followed by a comma (`,`) and then add your column information.\n",
    "\n",
    "**15\\.** Assign the values for all rows in the column `age` to a variable called `age` using `loc`. Run the second cell to confirm that this is the same as if you had used `df['age']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the values of the rows in the age column to a variable called age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "test = (age == df['age']).sum()\n",
    "if test == 303:\n",
    "    print('Both code versions produce the same results. There were 303 matching values in the columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTINCT\n",
    "It is very common for us to want to find the distinct values within a column. Whether this is the actual values, or the number of unique values, both are used frequently in SQL code. \n",
    "\n",
    "In pandas, instead of using the term \"distinct\", we use the term \"unique\". There are built in methods in pandas that allow us to do this. Both of these methods are specific to Series and not DataFrames. That is, we can only run these on a series, or a specific column within the DataFrame. \n",
    "\n",
    "- **`.unique()`:** Returns the unique values in a column/series\n",
    "- **`.nuniuqe()`:** Return the number of unique values in a column/series\n",
    "\n",
    "**16\\.** What are the distinct/unique values in the `slope` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><span style=\"color:blue\">Click to see my code</span></summary>\n",
    "\n",
    "```python\n",
    "df['slope'].unique()\n",
    "```\n",
    "\n",
    "The unique values in `slope` are 0, 1, and 2. Notice that this is not the way that they were presented - pandas will order them in the order it first encountered each value in the table.\n",
    "\n",
    "</details>\n",
    "\n",
    "**17\\.** How many unique values are in the `oldpeak` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><span style=\"color:blue\">Click to confirm your answer</span></summary>\n",
    "There are 40 unique values in the `oldpeak` column.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for now, but feel free to use the cells below (or add more) to play around with the different functionality you've learnt in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (data_analysis)",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
